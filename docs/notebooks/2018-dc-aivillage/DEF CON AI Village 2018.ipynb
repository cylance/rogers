{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rogers.logger import init_logging, logging, get_logger\n",
    "\n",
    "init_logging(level=logging.DEBUG)\n",
    "\n",
    "log = get_logger(\"rogers.notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rogers as rgr\n",
    "import rogers.config as cfg\n",
    "import pandas as pd\n",
    "import plotly\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from rogers.index.hnsw import Index as HNSW\n",
    "from rogers.index.pdci import Index as PDCI\n",
    "\n",
    "cfg.configure(\"./config.ini\")\n",
    "\n",
    "db = rgr.store.Database()\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv.gz\")\n",
    "df.groupby(['source'])['label'].count()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add label data from df as contextual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr.api.feature_add(df, 'CATEGORICAL', 'CONTEXTUAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feature extraction on file samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr.api.extract(filter_hashvals=df['sha256'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples, transform, and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(db.get_samples())\n",
    "pipeline = rgr.vectorizer.online_pe_pipeline()\n",
    "xs = pipeline.fit_transform(samples)\n",
    "hashvals = [s.sha256 for s in samples]\n",
    "joblib.dump([hashvals, xs], 'data.pk.gz')\n",
    "joblib.dump(pipeline, 'pipeline.pk.gz')\n",
    "\n",
    "# Reload exisiting samples and pipeline\n",
    "# hashvals, xs = joblib.load('data.pk.gz')\n",
    "# pipeline = joblib.load('pipeline.pk.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "Fit HSNW and PDCI index and persist. Parameters selected from basic grid search using a 90/10 split on dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_idx = HNSW(db=db, pipeline=pipeline, efConstruction=400, M=16)\n",
    "hnsw_idx.fit(xs, hashvals)\n",
    "hnsw_idx.save()\n",
    "# hnsw_idx.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdci_idx = PDCI(db=db, pipeline=pipeline, simple_indicies=20, composite_indices=2)\n",
    "pdci_idx.fit(xs, hashvals)\n",
    "pdci_idx.save()\n",
    "# pdci_idx.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Select a random sample and visualize using plotly. Neighbor graph weights edges by similarity. Change values of `k` for bringing back more results. Set `include_neighbors` to true queries neighbors returned in initial query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[np.random.choice(range(len(samples)), 1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hnsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = hnsw_idx.query_samples([sample], k=5, include_neighbors=True)\n",
    "print(\"%s has label  %s\" % (sample.sha256, sample.contextual_features()['label']))\n",
    "print()\n",
    "for ret in neighbors[0]['neighbors']:\n",
    "    print( ret[0].sha256, ret[1])\n",
    "rgr.visualize.plt_neighbor_graph(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pdci\n",
    "\n",
    "Query parameters have been selected from basic grid search. `d` is the intrinsic dimensionality of the samples and used as parameter for worst case bounds in `pdci`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_retrieve = pdci_idx.index.omega_k_retrieve(k=5, d=8, include_neighbors=True)\n",
    "k_visit = pdci_idx.index.omega_k_visit(k=5, d=8)\n",
    "\n",
    "neighbors = pdci_idx.query_samples([sample], k=5, include_neighbors=False, k_retrieve=k_retrieve, k_visit=k_visit)\n",
    "print(\"%s is in label  %s\" % (sample.sha256, sample.contextual_features()['label']))\n",
    "print()\n",
    "for ret in neighbors[0]['neighbors']:\n",
    "    print( ret[0].sha256, ret[1])\n",
    "rgr.visualize.plt_neighbor_graph(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xori Feature Extraction and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(500)\n",
    "df.groupby(['source', 'label'])['label'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Example mnemonics bag of words extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rogers.sample import Sample\n",
    "from rogers.sample.xori import Xori\n",
    "\n",
    "_, msg = Xori.process(\"var/samples/00/01/0D/97/00010D97E3B9BA14D1A1EB21197918A42DA58B1291B810A68FC7DC17D1BAF3A2\")\n",
    "sample = Sample.deserialize(msg)\n",
    "sample.get('mnemonics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feature extraction on file samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgr.api.extract(filter_hashvals=df['sha256'].tolist(), sample_class=Xori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from rogers.vectorizer.BaseVectorizer import BaseVectorizer\n",
    "from rogers.vectorizer import SignatureVectorizer, HeaderVectorizer, SymImportsVectorizer, SymExportsVectorizer\n",
    "\n",
    "class MnemonicVectorizer(BaseVectorizer):\n",
    "\n",
    "    def explode(self, s):\n",
    "        \"\"\" Preprocess sample for vectorizers\n",
    "        :param s: Sample instance\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mnemonics = s.get('mnemonics')\n",
    "        return mnemonics if isinstance(mnemonics, dict) else {}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorize', FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('signatures', Pipeline([\n",
    "                    ('vectorizer', SignatureVectorizer(TfidfVectorizer(sublinear_tf=True, min_df=2, max_df=0.90))),\n",
    "                ])),\n",
    "                ('mnemonics', Pipeline([\n",
    "                    ('vectorizer', MnemonicVectorizer()),\n",
    "                    ('normalize', Normalizer())\n",
    "                ])),\n",
    "                ('header', Pipeline([\n",
    "                    ('vectorizer', HeaderVectorizer()),\n",
    "                    ('normalize', Normalizer())\n",
    "                ])),\n",
    "                ('sym_imports', Pipeline([\n",
    "                    ('vectorizer', SymImportsVectorizer(TfidfVectorizer(sublinear_tf=True, min_df=2, max_df=0.90))),\n",
    "                    ('projection', TruncatedSVD(n_components=256)),\n",
    "                ]))\n",
    "            ],\n",
    "        )),\n",
    "        ('projection', TruncatedSVD(n_components=128)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(db.get_samples())\n",
    "xs = pipeline.fit_transform(samples)\n",
    "hashvals = np.array([s.sha256 for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = HNSW(db=db, pipeline=pipeline, n_esimators=20)\n",
    "# fit the index and save\n",
    "idx.fit(xs, hashvals)\n",
    "idx.save()\n",
    "# idx.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random sample\n",
    "sample = samples[np.random.choice(range(len(samples)), 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = idx.query_samples([sample], k=10, include_neighbors=True)\n",
    "print(\"%s has label  %s\" % (sample.sha256, sample.contextual_features()['label']))\n",
    "print()\n",
    "for ret in neighbors[0]['neighbors']:\n",
    "    print( ret[0].sha256, ret[1])\n",
    "rgr.visualize.plt_neighbor_graph(neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
